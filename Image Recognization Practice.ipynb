{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "MBAN6500_assignment2_Ruoxi_Jia.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Collinjia/Image_recognization/blob/main/Image%20Recognization%20Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik-Zi9PNhMPc"
      },
      "source": [
        "# Image Recognization Practice\n",
        "\n",
        "## Installation\n",
        "Instructions can be found here:\n",
        "* [Tensorflow](https://www.tensorflow.org/install/)\n",
        "\n",
        "Since Tensorflow 2.0, Keras is included in Tensorflow and will be automatically installed with Tensorflow. It can be accessed as ```tensorflow.keras```\n",
        "\n",
        "I recommend installing using ```pip```. For Tensorflow is it sufficient to install the CPU version. The GPU version requires a good workstation with high-end Nvidia GPU(s), and it is not necessary for this tutorial.\n",
        "\n",
        "If you're using a virtualenv:\n",
        "```\n",
        "pip3 install tensorflow\n",
        "```\n",
        "Add ```sudo``` for a systemwide installation (i.e. no ```virtualenv```).\n",
        "```\n",
        "sudo pip3 install tensorflow\n",
        "```\n",
        "Make sure that you have ```sklearn```, ```matplotlib``` and ```numpy``` installed, too.\n",
        "\n",
        "\n",
        "## Part 1 - understand a model\n",
        "\n",
        "### Optimizers\n",
        "\n",
        "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater than zero. The goal of training a model is to find a set of weights and biases (i.e. parameters) that have, on average, a low loss across all examples. The term cost is used interchangably with loss. See the [loss section](https://keras.io/losses/) in the Keras documentation for a list and descriptions of what is available.\n",
        "\n",
        "![Side by side loss](https://drive.google.com/uc?id=1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae)\n",
        "<b>Figure 1.</b> Left: high loss and right: low loss.\n",
        "\n",
        "<!-- https://drive.google.com/file/d/1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae/view?usp=sharing\n",
        "<img src=\"./fig/LossSideBySide.png\" width=\"500\">\n",
        "<figcaption>Figure. Left: high loss and right: low loss.</figcaption>\n",
        " -->\n",
        "The optimizer is the algorithm used to minimize the loss/cost. Optimizers in neural networks work by finding the gradient/derivative of the loss with respect to the parameters (i.e. the weights). \"Gradient\" is the correct term since a we are looking at multi-dimensional systems (i.e. many parameters), however, the terms are often used interchangably. For those who didn't take multivariate calculus, just think of the gradient as a derivative. The derivative of the loss with respect to a parameters tells us how much the loss changes when we nudge a weight up or down. So, by knowing how a given parameter affects the loss the optimizer can change it so as to decrease the loss. The various optimizers differ in how they change the weights. \n",
        "\n",
        "#### Mini-overview over popular optimizers\n",
        "\n",
        "* **Stochastic Gradient Descent (SGD)**. This is the most basic and easy to understand optimizer. It updates the weights in the negative direction of the gradient by taking the average gradient of mini-batch of data (e.g. 20-1000 examples) in each step. Vanilla SGD only has one hyper-parameter, the learning rate.\n",
        "* **Momentum**. This optimizer \"gains speed\" when the gradient has pointed in the same direction for several consecutive updates. That is, it has a momentum and want to keep moving in that direction. It gains momentum by accumulating an exponentially decaying moving average of past gradients. The step size depends on how large and aligned the sequence of gradients are. The most important hyper-parameter is alpha and common values are 0.5 and 0.9.\n",
        "* **Nesterov Momentum**. This is a modification of the standard momentum optimizer.\n",
        "* **AdaGrad**. This optimizer Ada-ptively sets the learning rate depending on the steepness/magnitude of the Grad-ients. This is done so that weights with big gradients get a smaller effective learning rate, and weights with small gradients will get a greater effective learning rate. The result is quicker progress in the more gently sloped directions of the weight space and a slowdown in stepp regions.\n",
        "* **RMSProp**. This is modification of AdaGrad, where the accumulated gradient decays, that is, the influence of previous gradients gradually decreases.\n",
        "* **Adam**. The name comes from \"adaptive moments\", and it is a combination of RMSProp and momentum. It has several hyper-parameters.\n",
        "\n",
        "The above list just gives a quick overview of some of the most common. However, old optimizers are constantly improved and new are developed. SGD and momentum are most basic and easiest to understand and implement. They are still in use, but the more advanced optimizers tend to be better for practical use. Which one to use is generally an emperical question depending on both the data and the model.\n",
        "\n",
        "For a more complete overview of optimization algorithms see [this comparison](http://ruder.io/optimizing-gradient-descent/), and to see what is available in Keras, see the [optimizer section](https://keras.io/optimizers/) of the documentation.\n",
        "\n",
        "See the images below for a comparison of optimizers in a 2D space (NAG: Nesterov accelerated gradient, Adadelta: an extension of AdaGrad).\n",
        "\n",
        "![Contours - optimizer comparison](https://drive.google.com/uc?id=1CmrD-UPZ7EIUjRuO_ib7k9CL1FO2bbLk)\n",
        "<b>Figure 2.</b> Comparison of six different optimizers.\n",
        "\n",
        "\n",
        "![Saddle point - optimizer comparison](https://drive.google.com/uc?id=1QVhN9rAvCjXtGyNZkmFivyyCzNsntObh)\n",
        "<b>Figure 3.</b> Comparison of six different optimizers at a saddle point.\n",
        "\n",
        "<!-- <img src=\"./fig/contours_evaluation_optimizers.gif\" width=\"500\">\n",
        "<img src=\"./fig/saddle_point_evaluation_optimizers.gif\" width=\"500\"> -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3rDIHUthMPg"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "# for the random seed\n",
        "import tensorflow as tf\n",
        "\n",
        "# set the random seeds to get reproducible results\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X, y = X[:1000], y[:1000]\n",
        "X = X.reshape(X.shape[0], 28, 28, 1)\n",
        "# Normalize\n",
        "X = X / 255.\n",
        "# number of unique classes\n",
        "num_classes = len(np.unique(y))\n",
        "y = y.astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
        "\n",
        "num_tot = y.shape[0]\n",
        "num_train = y_train.shape[0]\n",
        "num_test = y_test.shape[0]\n",
        "\n",
        "y_oh = np.zeros((num_tot, num_classes))\n",
        "y_oh[range(num_tot), y] = 1\n",
        "\n",
        "y_oh_train = np.zeros((num_train, num_classes))\n",
        "y_oh_train[range(num_train), y_train] = 1\n",
        "\n",
        "y_oh_test = np.zeros((num_test, num_classes))\n",
        "y_oh_test[range(num_test), y_test] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqphEuoNhMPh"
      },
      "source": [
        "### Question 1\n",
        "**The data set**\n",
        "\n",
        "Plot a three examples from the data set.\n",
        "* What type of data are in the data set?\n",
        "\n",
        "  <font color='blue'> All data are 28*28 images. </font>\n",
        "    \n",
        "\n",
        "* What does the line ```X = X.reshape(X.shape[0], 28, 28, 1)``` do?\n",
        "\n",
        "  <font color='blue'> Reshape the numpy array from (1000, 784) to (1000, 28, 28, 1) so that it can be taken as the input layer. Because there are 1000 images and each image is 28*28.</font>\n",
        "\n",
        "Look at how the encoding of the targets (i.e. ```y```) is changed. E.g. the lines\n",
        "```\n",
        "    y_oh = np.zeros((num_tot, num_classes))\n",
        "    y_oh[range(num_tot), y] = 1\n",
        "```\n",
        "Print out a few rows of ```y``` next to ```y_oh```.\n",
        "* What is the relationship between ```y``` and ```y_oh```?\n",
        "\n",
        "  <font color='blue'>y_oh is the one-hot encoding matrix form of y. If y[i] is n, then y_oh will have 1 on the nth number and all else zero in the i row. </font> \n",
        "    \n",
        "* What is the type of encoding in ```y_oh``` called and why is it used?\n",
        "\n",
        "  <font color='blue'> One-hot encoding. Because we need to code for categorical values without giving them an order. </font>\n",
        "    \n",
        "    \n",
        "* Plot three data examples in the same figure and set the correct label as title. \n",
        "    * It should be possible to see what the data represent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqGlAN_R-JHO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "b2e360c4-77ef-48fa-d7c8-c94bf26753fb"
      },
      "source": [
        "# print out first 5 rows of y and y_oh.\n",
        "print(\"first 5 elements of y: \",y[:5])\n",
        "print(\"first 5 elements of y_oh: \")\n",
        "print(y_oh[:5])\n",
        "\n",
        "# Show the data example of the first three data\n",
        "# Reshape the X into 28*28 images.\n",
        "X_plot = X.reshape(X.shape[0], 28, 28)\n",
        "# create the empty list for subplot\n",
        "ax = []\n",
        "rows = 1\n",
        "cols = 3\n",
        "fig=plt.figure()\n",
        "\n",
        "for i in range(rows*cols):\n",
        "    # create the subplot and append to ax\n",
        "    ax.append(fig.add_subplot(rows,cols,i+1))\n",
        "    # set the title of subplots\n",
        "    ax[-1].set_title(y[i])\n",
        "    plt.imshow(X_plot[i])\n",
        "    # Increase the space between subplots\n",
        "    fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first 5 elements of y:  [5 0 4 1 9]\n",
            "first 5 elements of y_oh: \n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLh1MIVbhMPh"
      },
      "source": [
        "### Question 2\n",
        "**The model**\n",
        "\n",
        "Below is some code for bulding and training a model with Keras.\n",
        "* What type of network is implemented below? I.e. a normal MLP, RNN, CNN, Logistic Regression...?\n",
        "\n",
        "  <font color='blue'> CNN </font>\n",
        "    \n",
        "    \n",
        "* What does ```Dropout()``` do?\n",
        "\n",
        "  <font color='blue'> The code add a dropout layer into the model. The dropout layer randomly sets input units to 0 with a frequency of given rate.It randomly remove neurons to prevent overfitting. </font>\n",
        "\n",
        "\n",
        "* Which type of activation function is used for the hidden layers?\n",
        "\n",
        "  <font color='blue'> Relu function </font>\n",
        "\n",
        "\n",
        "* Which type of activation function is used for the output layer?\n",
        "\n",
        "  <font color='blue'> softmax function </font>\n",
        "\n",
        "\n",
        "* Why are two different activation functions used?\n",
        "\n",
        "  <font color='blue'> Normally we choose different activision function for hidden layers and output layer. For hidden layers, it depends on the type of neurual network. Since this network is CNN structure, we choose ReLU activition.\n",
        "    \n",
        "    For output layer, it depends on the what prediction that we want to make. Since this network is design for the multiclass classification, we choose softmax activation.\n",
        "  </font>\n",
        "\n",
        "\n",
        "* What optimizer is used in the model below?\n",
        "\n",
        "  <font color='blue'> Nesterov momentum SGD optimizer </font>\n",
        "\n",
        "\n",
        "* How often are the weights updated (i.e. after how many data examples)?\n",
        "\n",
        "  <font color='blue'> 32. Because the batch size = 32. </font>\n",
        "\n",
        "\n",
        "* What loss function is used?\n",
        "\n",
        "  <font color='blue'> categorical crossentropy </font>\n",
        "\n",
        "\n",
        "* How many parameters (i.e. weights and biases, NOT hyper-parameters) does the model have?\n",
        "\n",
        "  <font color='blue'> 108,618 parameters </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OvVjyUThhMPi"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=60)\n",
        "\n",
        "# Evaluate performance\n",
        "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
        "\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJboUaNfW0JF"
      },
      "source": [
        "# show the summary of the model.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKZRC_4xhMPi"
      },
      "source": [
        "## Part 2 - train a model\n",
        "\n",
        "A model's performance depends on many factors apart from the model architecture (e.g. type and number of layers) and the dataset. Here you will get to explore some of the factors that affect model performance. Much of the skill in training deep learning models lies in quickly finding good values/options for these choises.\n",
        "\n",
        "In order to observe the learning process it is best to compare the training set loss with the loss on the test set. How to visualize these variables with Keras is described under [Training history visualization](https://keras.io/visualization/#training-history-visualization) in the documentation.\n",
        "\n",
        "You will explore the effect of 1) optimizer, 2) training duration, and 3) dropout (see the question above).\n",
        "\n",
        "When training, an **epoch** is one pass through the full training set.\n",
        "\n",
        "### Question 3\n",
        "\n",
        "* **Vizualize the training**. Use the model above to observe the training process. Train it for 150 epochs and then plot both \"loss\" and \"val_loss\" (i.e. loss on the valiadtion set, here the terms \"validation set\" and \"test set\" are used interchangably, but this is not always true). What is the optimal number of epochs for minimizing the test set loss? \n",
        "    * Remember to first reset the weights (this can be done by calling ```model.compile()```), otherwise the training just continues from where it was stopped earlier.\n",
        "  \n",
        "  <font color='blue'> In this model, the test set loss is minimized in epochs 9 with the value = 0.2801</font>\n",
        "\n",
        "\n",
        "* **Optimizer**. Select three different optimizers and for each find the close-to-optimal hyper-parameter(s). In your answer, include a) your three choises, b) best hyper-parameters for each of the three optimizers and, c) the code that produced the results.\n",
        "    * *NOTE* that how long the training takes varies with optimizer. I.e., make sure that the model is trained for long enough to reach optimal performance.\n",
        "\n",
        "  <font color='blue'> a) Optimizer chosen: SGD, Nesterov Momentum SGD and Adam</font>\n",
        "\n",
        "  <font color='blue'>\n",
        "  b) For SGD, I use epochs = 150. And for Nesterov and Adam, since they are learning faster, I decrease the epochs to 60.\n",
        "\n",
        "  *   For SGD, the optimal learning rate is 0.01 with validation accuracy=0.9250.\n",
        "  *   For Nesterov Momentum SGD, the optimal learning rate is 0.01 and momentum is 0.5 with validation accuracy=0.9300.\n",
        "  *   For Adam, the optimal learning rate is 0.01 with validation accuracy=0.9600.\n",
        "\n",
        "  So the best optimizer is Adam.\n",
        "\n",
        "  </font>\n",
        "\n",
        "* **Dropout**. Use the best optimizer and do hyper-parameter seach and find the best value for ```Dropout()```.\n",
        "  \n",
        "  <font color='blue'> \n",
        "  Based on the result before, we use Adam optimizer with learning rate=0.01 to get the optimal dropout rate.\n",
        "\n",
        "  The best value for dropout rate=0.5 with validation accuracy = 0.9675.\n",
        "  \n",
        "  </font>\n",
        "\n",
        "* **Best model**. Combine the what you learned from the above three questions to build the best model. How much better is it than the worst and average models?\n",
        "\n",
        "  <font color='blue'> The best model (Adam with lr = 0.01 and dropout rate = 0.5) has an accuracy rate of 0.9675, which is better than the initial one with accuracy rate of 0.9250. </font>\n",
        "\n",
        "\n",
        "* **Results on the test set**. When doing this search for good model configuration/hyper-parameter values, the data set was split into *two* parts: a training set and a test set (the term \"validation\" was used interchangably wiht \"test\"). For your final model, is the performance (i.e. accuracy) on the test set representative for the performance one would expect on a previously unseen data set (drawn from the same distribution)? Why?\n",
        "\n",
        "  <font color='blue'> Yes, the test set performance will represent the performance expected on the unseen data set. Because they are drawn from the same distribution. </font>\n",
        "\n",
        "\n",
        "## Further information\n",
        "For ideas about hyper-parameter tuning, take a look at the strategies described in the sklearn documentation under [model selection](https://scikit-learn.org/stable/model_selection.html), or in this [blog post](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html) from TensorFlow. For a more thorough discussion about optimizers see [this video](https://www.youtube.com/watch?v=DiNzQP7kK-s) discussing the article [Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers](https://arxiv.org/abs/2007.01547).\n",
        "\n",
        "\n",
        "**Good luck!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuprlIQ1NXU3"
      },
      "source": [
        "# install the keras tuner for parameter tuning\n",
        "%pip install -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vg7LZqnK6no"
      },
      "source": [
        "### Vizualize the Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0q6VTyNhMPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52b83e32-686e-462e-fb2a-ea1270bd143c"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_oh_train, \n",
        "                    batch_size=32, epochs=150,\n",
        "                    validation_data = (X_test, y_oh_test))\n",
        "\n",
        "# Visualize the training\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "ax0 = fig.add_subplot(121)\n",
        "# plot the training history for loss\n",
        "ax0.plot(history.history['val_loss'], label='validation loss')\n",
        "ax0.plot(history.history['loss'], label='training loss')\n",
        "ax0.set_ylabel('Loss')\n",
        "ax0.set_xlabel('Epoch')\n",
        "ax0.legend()\n",
        "# plot the training history for accuracy\n",
        "ax1 = fig.add_subplot(122)\n",
        "ax1.plot(history.history['val_accuracy'], label='validation accuracy')\n",
        "ax1.plot(history.history['accuracy'], label='training accuracy')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.legend()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "25/25 [==============================] - 2s 38ms/step - loss: 2.2322 - accuracy: 0.2112 - val_loss: 1.8012 - val_accuracy: 0.4500\n",
            "Epoch 2/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.3094 - accuracy: 0.6463 - val_loss: 0.7979 - val_accuracy: 0.7600\n",
            "Epoch 3/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.5476 - accuracy: 0.8214 - val_loss: 0.4619 - val_accuracy: 0.8600\n",
            "Epoch 4/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.3458 - accuracy: 0.9075 - val_loss: 0.3852 - val_accuracy: 0.8900\n",
            "Epoch 5/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.2481 - accuracy: 0.9129 - val_loss: 0.3712 - val_accuracy: 0.8950\n",
            "Epoch 6/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1618 - accuracy: 0.9439 - val_loss: 0.3957 - val_accuracy: 0.8900\n",
            "Epoch 7/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1702 - accuracy: 0.9457 - val_loss: 0.3445 - val_accuracy: 0.9100\n",
            "Epoch 8/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1203 - accuracy: 0.9568 - val_loss: 0.3616 - val_accuracy: 0.8850\n",
            "Epoch 9/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0991 - accuracy: 0.9666 - val_loss: 0.2801 - val_accuracy: 0.9250\n",
            "Epoch 10/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0544 - accuracy: 0.9869 - val_loss: 0.3497 - val_accuracy: 0.9150\n",
            "Epoch 11/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0416 - accuracy: 0.9927 - val_loss: 0.3288 - val_accuracy: 0.9100\n",
            "Epoch 12/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0405 - accuracy: 0.9939 - val_loss: 0.3714 - val_accuracy: 0.9100\n",
            "Epoch 13/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0346 - accuracy: 0.9949 - val_loss: 0.3471 - val_accuracy: 0.9250\n",
            "Epoch 14/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0197 - accuracy: 0.9994 - val_loss: 0.3853 - val_accuracy: 0.9300\n",
            "Epoch 15/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0155 - accuracy: 0.9998 - val_loss: 0.3575 - val_accuracy: 0.9350\n",
            "Epoch 16/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0179 - accuracy: 0.9965 - val_loss: 0.3699 - val_accuracy: 0.9300\n",
            "Epoch 17/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.3656 - val_accuracy: 0.9200\n",
            "Epoch 18/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.3697 - val_accuracy: 0.9300\n",
            "Epoch 19/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 0.9250\n",
            "Epoch 20/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.3905 - val_accuracy: 0.9300\n",
            "Epoch 21/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 0.9300\n",
            "Epoch 22/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3926 - val_accuracy: 0.9250\n",
            "Epoch 23/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3807 - val_accuracy: 0.9300\n",
            "Epoch 24/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3971 - val_accuracy: 0.9300\n",
            "Epoch 25/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3936 - val_accuracy: 0.9300\n",
            "Epoch 26/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4004 - val_accuracy: 0.9250\n",
            "Epoch 27/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.9200\n",
            "Epoch 28/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4065 - val_accuracy: 0.9200\n",
            "Epoch 29/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4090 - val_accuracy: 0.9300\n",
            "Epoch 30/150\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4124 - val_accuracy: 0.9250\n",
            "Epoch 31/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4153 - val_accuracy: 0.9250\n",
            "Epoch 32/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
            "Epoch 33/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4190 - val_accuracy: 0.9250\n",
            "Epoch 34/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4220 - val_accuracy: 0.9300\n",
            "Epoch 35/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4268 - val_accuracy: 0.9250\n",
            "Epoch 36/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4263 - val_accuracy: 0.9250\n",
            "Epoch 37/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4316 - val_accuracy: 0.9300\n",
            "Epoch 38/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4324 - val_accuracy: 0.9250\n",
            "Epoch 39/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4366 - val_accuracy: 0.9300\n",
            "Epoch 40/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4361 - val_accuracy: 0.9300\n",
            "Epoch 41/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4385 - val_accuracy: 0.9250\n",
            "Epoch 42/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 9.5389e-04 - accuracy: 1.0000 - val_loss: 0.4395 - val_accuracy: 0.9300\n",
            "Epoch 43/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 8.7589e-04 - accuracy: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.9300\n",
            "Epoch 44/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 7.5664e-04 - accuracy: 1.0000 - val_loss: 0.4456 - val_accuracy: 0.9250\n",
            "Epoch 45/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 7.8364e-04 - accuracy: 1.0000 - val_loss: 0.4449 - val_accuracy: 0.9300\n",
            "Epoch 46/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 8.6218e-04 - accuracy: 1.0000 - val_loss: 0.4479 - val_accuracy: 0.9250\n",
            "Epoch 47/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 7.4382e-04 - accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.9300\n",
            "Epoch 48/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.9547e-04 - accuracy: 1.0000 - val_loss: 0.4526 - val_accuracy: 0.9300\n",
            "Epoch 49/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 7.4040e-04 - accuracy: 1.0000 - val_loss: 0.4510 - val_accuracy: 0.9250\n",
            "Epoch 50/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.5876e-04 - accuracy: 1.0000 - val_loss: 0.4539 - val_accuracy: 0.9300\n",
            "Epoch 51/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.1762e-04 - accuracy: 1.0000 - val_loss: 0.4535 - val_accuracy: 0.9300\n",
            "Epoch 52/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 5.8019e-04 - accuracy: 1.0000 - val_loss: 0.4560 - val_accuracy: 0.9300\n",
            "Epoch 53/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 5.5301e-04 - accuracy: 1.0000 - val_loss: 0.4563 - val_accuracy: 0.9300\n",
            "Epoch 54/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 5.6151e-04 - accuracy: 1.0000 - val_loss: 0.4596 - val_accuracy: 0.9300\n",
            "Epoch 55/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.9891e-04 - accuracy: 1.0000 - val_loss: 0.4594 - val_accuracy: 0.9300\n",
            "Epoch 56/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.1614e-04 - accuracy: 1.0000 - val_loss: 0.4634 - val_accuracy: 0.9300\n",
            "Epoch 57/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.2972e-04 - accuracy: 1.0000 - val_loss: 0.4632 - val_accuracy: 0.9300\n",
            "Epoch 58/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.1560e-04 - accuracy: 1.0000 - val_loss: 0.4641 - val_accuracy: 0.9300\n",
            "Epoch 59/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 5.8387e-04 - accuracy: 1.0000 - val_loss: 0.4670 - val_accuracy: 0.9300\n",
            "Epoch 60/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 5.5804e-04 - accuracy: 1.0000 - val_loss: 0.4677 - val_accuracy: 0.9300\n",
            "Epoch 61/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.0936e-04 - accuracy: 1.0000 - val_loss: 0.4698 - val_accuracy: 0.9300\n",
            "Epoch 62/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.0649e-04 - accuracy: 1.0000 - val_loss: 0.4693 - val_accuracy: 0.9300\n",
            "Epoch 63/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 5.1828e-04 - accuracy: 1.0000 - val_loss: 0.4691 - val_accuracy: 0.9300\n",
            "Epoch 64/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.5772e-04 - accuracy: 1.0000 - val_loss: 0.4727 - val_accuracy: 0.9300\n",
            "Epoch 65/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.4826e-04 - accuracy: 1.0000 - val_loss: 0.4718 - val_accuracy: 0.9300\n",
            "Epoch 66/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.1833e-04 - accuracy: 1.0000 - val_loss: 0.4745 - val_accuracy: 0.9300\n",
            "Epoch 67/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.9075e-04 - accuracy: 1.0000 - val_loss: 0.4746 - val_accuracy: 0.9300\n",
            "Epoch 68/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.1723e-04 - accuracy: 1.0000 - val_loss: 0.4766 - val_accuracy: 0.9300\n",
            "Epoch 69/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.1212e-04 - accuracy: 1.0000 - val_loss: 0.4777 - val_accuracy: 0.9300\n",
            "Epoch 70/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.0052e-04 - accuracy: 1.0000 - val_loss: 0.4790 - val_accuracy: 0.9300\n",
            "Epoch 71/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.7851e-04 - accuracy: 1.0000 - val_loss: 0.4787 - val_accuracy: 0.9300\n",
            "Epoch 72/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.8828e-04 - accuracy: 1.0000 - val_loss: 0.4800 - val_accuracy: 0.9300\n",
            "Epoch 73/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.3602e-04 - accuracy: 1.0000 - val_loss: 0.4815 - val_accuracy: 0.9300\n",
            "Epoch 74/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.7779e-04 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.9300\n",
            "Epoch 75/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.4412e-04 - accuracy: 1.0000 - val_loss: 0.4830 - val_accuracy: 0.9300\n",
            "Epoch 76/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.4849e-04 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.9300\n",
            "Epoch 77/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.6576e-04 - accuracy: 1.0000 - val_loss: 0.4849 - val_accuracy: 0.9300\n",
            "Epoch 78/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.0817e-04 - accuracy: 1.0000 - val_loss: 0.4878 - val_accuracy: 0.9300\n",
            "Epoch 79/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.8178e-04 - accuracy: 1.0000 - val_loss: 0.4871 - val_accuracy: 0.9300\n",
            "Epoch 80/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.5103e-04 - accuracy: 1.0000 - val_loss: 0.4886 - val_accuracy: 0.9300\n",
            "Epoch 81/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.9750e-04 - accuracy: 1.0000 - val_loss: 0.4884 - val_accuracy: 0.9300\n",
            "Epoch 82/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.7603e-04 - accuracy: 1.0000 - val_loss: 0.4894 - val_accuracy: 0.9300\n",
            "Epoch 83/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.4639e-04 - accuracy: 1.0000 - val_loss: 0.4906 - val_accuracy: 0.9300\n",
            "Epoch 84/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.7722e-04 - accuracy: 1.0000 - val_loss: 0.4917 - val_accuracy: 0.9300\n",
            "Epoch 85/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.8956e-04 - accuracy: 1.0000 - val_loss: 0.4930 - val_accuracy: 0.9300\n",
            "Epoch 86/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.1681e-04 - accuracy: 1.0000 - val_loss: 0.4936 - val_accuracy: 0.9300\n",
            "Epoch 87/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.8091e-04 - accuracy: 1.0000 - val_loss: 0.4934 - val_accuracy: 0.9300\n",
            "Epoch 88/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 2.9636e-04 - accuracy: 1.0000 - val_loss: 0.4946 - val_accuracy: 0.9300\n",
            "Epoch 89/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 3.0495e-04 - accuracy: 1.0000 - val_loss: 0.4956 - val_accuracy: 0.9300\n",
            "Epoch 90/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.8726e-04 - accuracy: 1.0000 - val_loss: 0.4965 - val_accuracy: 0.9300\n",
            "Epoch 91/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.6299e-04 - accuracy: 1.0000 - val_loss: 0.4974 - val_accuracy: 0.9300\n",
            "Epoch 92/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.1881e-04 - accuracy: 1.0000 - val_loss: 0.4977 - val_accuracy: 0.9300\n",
            "Epoch 93/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.7435e-04 - accuracy: 1.0000 - val_loss: 0.4989 - val_accuracy: 0.9300\n",
            "Epoch 94/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.7377e-04 - accuracy: 1.0000 - val_loss: 0.4995 - val_accuracy: 0.9300\n",
            "Epoch 95/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.2369e-04 - accuracy: 1.0000 - val_loss: 0.4997 - val_accuracy: 0.9300\n",
            "Epoch 96/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.3670e-04 - accuracy: 1.0000 - val_loss: 0.5009 - val_accuracy: 0.9300\n",
            "Epoch 97/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.7989e-04 - accuracy: 1.0000 - val_loss: 0.5013 - val_accuracy: 0.9300\n",
            "Epoch 98/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.5873e-04 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.9300\n",
            "Epoch 99/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.4108e-04 - accuracy: 1.0000 - val_loss: 0.5033 - val_accuracy: 0.9300\n",
            "Epoch 100/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.3307e-04 - accuracy: 1.0000 - val_loss: 0.5041 - val_accuracy: 0.9300\n",
            "Epoch 101/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.3325e-04 - accuracy: 1.0000 - val_loss: 0.5037 - val_accuracy: 0.9300\n",
            "Epoch 102/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.1144e-04 - accuracy: 1.0000 - val_loss: 0.5055 - val_accuracy: 0.9300\n",
            "Epoch 103/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.6997e-04 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.9300\n",
            "Epoch 104/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.8910e-04 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.9300\n",
            "Epoch 105/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.6246e-04 - accuracy: 1.0000 - val_loss: 0.5066 - val_accuracy: 0.9300\n",
            "Epoch 106/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.1367e-04 - accuracy: 1.0000 - val_loss: 0.5091 - val_accuracy: 0.9300\n",
            "Epoch 107/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.5597e-04 - accuracy: 1.0000 - val_loss: 0.5088 - val_accuracy: 0.9300\n",
            "Epoch 108/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.6825e-04 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 0.9300\n",
            "Epoch 109/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.3434e-04 - accuracy: 1.0000 - val_loss: 0.5098 - val_accuracy: 0.9300\n",
            "Epoch 110/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.2652e-04 - accuracy: 1.0000 - val_loss: 0.5103 - val_accuracy: 0.9300\n",
            "Epoch 111/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.4359e-04 - accuracy: 1.0000 - val_loss: 0.5102 - val_accuracy: 0.9300\n",
            "Epoch 112/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.1304e-04 - accuracy: 1.0000 - val_loss: 0.5119 - val_accuracy: 0.9300\n",
            "Epoch 113/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 2.0752e-04 - accuracy: 1.0000 - val_loss: 0.5131 - val_accuracy: 0.9300\n",
            "Epoch 114/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 2.1596e-04 - accuracy: 1.0000 - val_loss: 0.5127 - val_accuracy: 0.9300\n",
            "Epoch 115/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.9781e-04 - accuracy: 1.0000 - val_loss: 0.5130 - val_accuracy: 0.9300\n",
            "Epoch 116/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.4725e-04 - accuracy: 1.0000 - val_loss: 0.5141 - val_accuracy: 0.9300\n",
            "Epoch 117/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.9921e-04 - accuracy: 1.0000 - val_loss: 0.5153 - val_accuracy: 0.9300\n",
            "Epoch 118/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.3732e-04 - accuracy: 1.0000 - val_loss: 0.5158 - val_accuracy: 0.9300\n",
            "Epoch 119/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.8265e-04 - accuracy: 1.0000 - val_loss: 0.5161 - val_accuracy: 0.9300\n",
            "Epoch 120/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.0496e-04 - accuracy: 1.0000 - val_loss: 0.5165 - val_accuracy: 0.9300\n",
            "Epoch 121/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.9879e-04 - accuracy: 1.0000 - val_loss: 0.5172 - val_accuracy: 0.9300\n",
            "Epoch 122/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.9958e-04 - accuracy: 1.0000 - val_loss: 0.5179 - val_accuracy: 0.9300\n",
            "Epoch 123/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.2096e-04 - accuracy: 1.0000 - val_loss: 0.5179 - val_accuracy: 0.9300\n",
            "Epoch 124/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.3130e-04 - accuracy: 1.0000 - val_loss: 0.5187 - val_accuracy: 0.9300\n",
            "Epoch 125/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.7749e-04 - accuracy: 1.0000 - val_loss: 0.5200 - val_accuracy: 0.9300\n",
            "Epoch 126/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.0541e-04 - accuracy: 1.0000 - val_loss: 0.5192 - val_accuracy: 0.9300\n",
            "Epoch 127/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.7254e-04 - accuracy: 1.0000 - val_loss: 0.5198 - val_accuracy: 0.9300\n",
            "Epoch 128/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.8565e-04 - accuracy: 1.0000 - val_loss: 0.5213 - val_accuracy: 0.9300\n",
            "Epoch 129/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.9248e-04 - accuracy: 1.0000 - val_loss: 0.5212 - val_accuracy: 0.9300\n",
            "Epoch 130/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.6189e-04 - accuracy: 1.0000 - val_loss: 0.5218 - val_accuracy: 0.9300\n",
            "Epoch 131/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.7072e-04 - accuracy: 1.0000 - val_loss: 0.5229 - val_accuracy: 0.9300\n",
            "Epoch 132/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.9506e-04 - accuracy: 1.0000 - val_loss: 0.5229 - val_accuracy: 0.9300\n",
            "Epoch 133/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.6713e-04 - accuracy: 1.0000 - val_loss: 0.5235 - val_accuracy: 0.9300\n",
            "Epoch 134/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.9237e-04 - accuracy: 1.0000 - val_loss: 0.5235 - val_accuracy: 0.9300\n",
            "Epoch 135/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.8200e-04 - accuracy: 1.0000 - val_loss: 0.5249 - val_accuracy: 0.9300\n",
            "Epoch 136/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.7290e-04 - accuracy: 1.0000 - val_loss: 0.5250 - val_accuracy: 0.9300\n",
            "Epoch 137/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.6697e-04 - accuracy: 1.0000 - val_loss: 0.5256 - val_accuracy: 0.9300\n",
            "Epoch 138/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.7292e-04 - accuracy: 1.0000 - val_loss: 0.5262 - val_accuracy: 0.9300\n",
            "Epoch 139/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.7031e-04 - accuracy: 1.0000 - val_loss: 0.5267 - val_accuracy: 0.9300\n",
            "Epoch 140/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.6408e-04 - accuracy: 1.0000 - val_loss: 0.5270 - val_accuracy: 0.9300\n",
            "Epoch 141/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.9832e-04 - accuracy: 1.0000 - val_loss: 0.5282 - val_accuracy: 0.9300\n",
            "Epoch 142/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.5125e-04 - accuracy: 1.0000 - val_loss: 0.5286 - val_accuracy: 0.9300\n",
            "Epoch 143/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.6105e-04 - accuracy: 1.0000 - val_loss: 0.5282 - val_accuracy: 0.9300\n",
            "Epoch 144/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.7872e-04 - accuracy: 1.0000 - val_loss: 0.5291 - val_accuracy: 0.9300\n",
            "Epoch 145/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.5360e-04 - accuracy: 1.0000 - val_loss: 0.5304 - val_accuracy: 0.9300\n",
            "Epoch 146/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.4425e-04 - accuracy: 1.0000 - val_loss: 0.5297 - val_accuracy: 0.9300\n",
            "Epoch 147/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.7598e-04 - accuracy: 1.0000 - val_loss: 0.5302 - val_accuracy: 0.9300\n",
            "Epoch 148/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.5149e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.9300\n",
            "Epoch 149/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.3969e-04 - accuracy: 1.0000 - val_loss: 0.5312 - val_accuracy: 0.9300\n",
            "Epoch 150/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.5543e-04 - accuracy: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.9300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f52cf895d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYcJRYarPaUr"
      },
      "source": [
        "### Hyperparameter Tuning - Optimizer: Basic Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTfnYYR0PfC0"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# Define the function that build the basic model for optimizer tuning, \n",
        "# which only add layers without complying.\n",
        "def build_basic_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGZfyD7MLIsd"
      },
      "source": [
        "### Hyperparameter Tuning - Optimizer: SGD\n",
        "Optimal Result: Learning rate = 0.01\n",
        "\n",
        "With validation Accuracy=0.9250"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQI1JGPVBd3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c5b503-dca4-4e75-fa7c-5ebbf0c5e67b"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Define the function that build sgd model\n",
        "def build_sgd_model(hp):\n",
        "    # Get the basic model\n",
        "    model = build_basic_model()\n",
        "    # Learning rate is chosen from three options: 0.01,0.001 and 0.0001\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=SGD(lr),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "        )\n",
        "    return model\n",
        "# Define the tuning procedure by RandomSearch method\n",
        "# The objective is to maximize the validation accuracy\n",
        "tuner = RandomSearch(\n",
        "    build_sgd_model,\n",
        "    objective='val_accuracy',\n",
        "    # This tuning procedure takes less than 5 trails.\n",
        "    # E.g. Keep everything else same, lr=0.01 is one trail\n",
        "    # lr=0.001 is another\n",
        "    max_trials=5,\n",
        "    # Execute the whole training procedure twice for every trail\n",
        "    executions_per_trial=2,\n",
        "    project_name=\"sgd_model\",\n",
        "    # Uncomment it if you need to re-train the model\n",
        "    # overwrite=True\n",
        ")\n",
        "# Search of the optimal hyperparameters by training the model\n",
        "tuner.search(X_train, y_oh_train, \n",
        "                batch_size=32, epochs=150,\n",
        "                validation_data = (X_test, y_oh_test))\n",
        "# Show the result of hyperparameter tuning.\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 02m 51s]\n",
            "val_accuracy: 0.8400000035762787\n",
            "\n",
            "Best val_accuracy So Far: 0.925000011920929\n",
            "Total elapsed time: 00h 08m 31s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in ./sgd_model\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.01\n",
            "Score: 0.925000011920929\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.001\n",
            "Score: 0.8400000035762787\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0001\n",
            "Score: 0.1875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Oyt8rBHiq2"
      },
      "source": [
        "### Hyperparameter Tuning - Optimizer: Nesterov Momentum SGD\n",
        "Optimal Result: Learning rate= 0.01, Momentum=0.5\n",
        "\n",
        "With Validation Accuracy = 0.9300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7LwjVrXHpU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8315961-89c6-4066-fddd-8a506cbd5a11"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Define the function that build the model\n",
        "def build_nesterov_model(hp):\n",
        "    # Get the basic model\n",
        "    model = build_basic_model()\n",
        "    # Learning rate is chosen from three options: 0.01,0.001 and 0.0001\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    # Momentum is chosen from two options: 0.5 and 0.9.\n",
        "    momentum = hp.Choice('momentum', [0.5, 0.9])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=SGD(lr, momentum=momentum, nesterov=True),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# Define the tuning procedure by RandomSearch method\n",
        "# The objective is to maximize the validation accuracy\n",
        "tuner = RandomSearch(\n",
        "    build_nesterov_model,\n",
        "    objective='val_accuracy',\n",
        "    # This tuning procedure takes less than 10 trails.\n",
        "    max_trials=10,\n",
        "    # Execute the whole training procedure twice for every trail\n",
        "    executions_per_trial=2,\n",
        "    project_name=\"nesterov_model\",\n",
        "    # Uncomment it if you need to re-train the model\n",
        "    # overwrite=True\n",
        ")\n",
        "# Search of the optimal hyperparameters by training the model\n",
        "tuner.search(X_train, y_oh_train, \n",
        "                batch_size=32, epochs=60,\n",
        "                validation_data = (X_test, y_oh_test))\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 6 Complete [00h 01m 08s]\n",
            "val_accuracy: 0.5974999964237213\n",
            "\n",
            "Best val_accuracy So Far: 0.9300000071525574\n",
            "Total elapsed time: 00h 06m 54s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in ./nesterov_model\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.01\n",
            "momentum: 0.5\n",
            "Score: 0.9300000071525574\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.01\n",
            "momentum: 0.9\n",
            "Score: 0.9275000095367432\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.001\n",
            "momentum: 0.9\n",
            "Score: 0.9149999916553497\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.001\n",
            "momentum: 0.5\n",
            "Score: 0.8149999976158142\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0001\n",
            "momentum: 0.9\n",
            "Score: 0.5974999964237213\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0001\n",
            "momentum: 0.5\n",
            "Score: 0.20749999582767487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJXfzf3nOOP1"
      },
      "source": [
        "### Hyperparameter Tuning - Optimizer: Adam\n",
        "Optimal Result: Learning rate = 0.01\n",
        "\n",
        "With Validation Accuracy = 0.9600"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIQGA7GHOY2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a35d09-894b-4092-9a84-841fca4f96ed"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Define the function that build the Adam model\n",
        "def build_adam_model(hp):\n",
        "    # Get the basic model\n",
        "    model = build_basic_model()\n",
        "    # Learning rate is chosen from three options: 0.01,0.001 and 0.0001\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(lr),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "        )\n",
        "    return model\n",
        "\n",
        "tuner=RandomSearch(\n",
        "    build_adam_model,\n",
        "    objective='val_accuracy',\n",
        "    # This tuning procedure takes less than 5 trails.\n",
        "    max_trials=5,\n",
        "    # Execute the whole training procedure twice for every trail\n",
        "    executions_per_trial=2,\n",
        "    project_name=\"adam_model\",\n",
        "    # Uncomment it if you need to re-train the model\n",
        "    # overwrite=True)\n",
        "\n",
        "# Search of the optimal hyperparameters by training the model\n",
        "tuner.search(X_train, y_oh_train, \n",
        "                batch_size=32, epochs=60,\n",
        "                validation_data = (X_test, y_oh_test))\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 01m 08s]\n",
            "val_accuracy: 0.9424999952316284\n",
            "\n",
            "Best val_accuracy So Far: 0.9599999785423279\n",
            "Total elapsed time: 00h 03m 25s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in ./adam_model\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.01\n",
            "Score: 0.9599999785423279\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.001\n",
            "Score: 0.9424999952316284\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0001\n",
            "Score: 0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCw6CXsOgS-z"
      },
      "source": [
        "### Hyperparameter Tuning - Dropout Rate\n",
        "Optimal Result: Dropout Rate = 0.5\n",
        "\n",
        "With Validation Accuracy=0.9675"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94na7ZFEg4p7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582c29e2-0ab0-4bff-ee42-c815af6fbb57"
      },
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Define the model tuning dropout rate\n",
        "def tune_dropout_model(hp):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # dropout rate is chosen from following 10 options.\n",
        "    drop_rate = hp.Choice('drop_rate',[0.0, 0.1, 0.2, 0.3, 0.4,\n",
        "                                       0.5, 0.6, 0.7, 0.8, 0.9])\n",
        "    \n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(drop_rate))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(drop_rate))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model with the previously found \n",
        "    # best optimizer=Adam and lr=0.001\n",
        "    model.compile(\n",
        "        optimizer=Adam(lr=0.01),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "        )\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    tune_dropout_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    project_name=\"dropout_model\",\n",
        "    # Uncomment it if you need to re-train the model\n",
        "    # overwrite=True\n",
        ")\n",
        "\n",
        "tuner.search(X_train, y_oh_train, \n",
        "                batch_size=32, epochs=60,\n",
        "                validation_data = (X_test, y_oh_test))\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 7 Complete [00h 01m 07s]\n",
            "val_accuracy: 0.9624999761581421\n",
            "\n",
            "Best val_accuracy So Far: 0.9675000011920929\n",
            "Total elapsed time: 00h 07m 58s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in ./dropout_model\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "drop_rate: 0.5\n",
            "Score: 0.9675000011920929\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "drop_rate: 0.3\n",
            "Score: 0.9650000035762787\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "drop_rate: 0.2\n",
            "Score: 0.9624999761581421\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "drop_rate: 0.1\n",
            "Score: 0.9599999785423279\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "drop_rate: 0.0\n",
            "Score: 0.9549999833106995\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "drop_rate: 0.7\n",
            "Score: 0.9399999976158142\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "drop_rate: 0.9\n",
            "Score: 0.10000000149011612\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}